{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import argparse\n",
    "\n",
    "import gensim\n",
    "from gensim.models import word2vec\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import LSTM, Bidirectional\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "from keras.utils import to_categorical\n",
    "#x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "from keras import backend as K\n",
    "from keras.callbacks import EarlyStopping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mygpu/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:2: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(128, return_sequences=True, input_shape=(398, 40), activation=\"sigmoid\", recurrent_activation=\"hard_sigmoid\")`\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(128,input_shape=(398,40),activation='sigmoid', inner_activation='hard_sigmoid', return_sequences=True))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dropout(0.2))\n",
    "#model.add(TimeDistributedDense(11))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pad_sequences(sequences, maxlen=None, dim=1, dtype='int32',\n",
    "    padding='pre', truncating='pre', value=0.):\n",
    "    '''\n",
    "        Override keras method to allow multiple feature dimensions.\n",
    "\n",
    "        @dim: input feature dimension (number of features per timestep)\n",
    "    '''\n",
    "    lengths = [len(s) for s in sequences]\n",
    "\n",
    "    nb_samples = len(sequences)\n",
    "    if maxlen is None:\n",
    "        maxlen = np.max(lengths)\n",
    "\n",
    "    x = (np.ones((nb_samples, maxlen, dim)) * value).astype(dtype)\n",
    "    for idx, s in enumerate(sequences):\n",
    "        if truncating == 'pre':\n",
    "            trunc = s[-maxlen:]\n",
    "        elif truncating == 'post':\n",
    "            trunc = s[:maxlen]\n",
    "        else:\n",
    "            raise ValueError(\"Truncating type '%s' not understood\" % padding)\n",
    "\n",
    "        if padding == 'post':\n",
    "            x[idx, :len(trunc)] = trunc\n",
    "        elif padding == 'pre':\n",
    "            x[idx, -len(trunc):] = trunc\n",
    "        else:\n",
    "            raise ValueError(\"Padding type '%s' not understood\" % padding)\n",
    "    return (x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mygpu/anaconda3/lib/python3.5/site-packages/keras/preprocessing/text.py:145: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n",
      "  warnings.warn('The `nb_words` argument in `Tokenizer` '\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer#,base_filter\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def shift(seq, n):\n",
    "    n = n % len(seq)\n",
    "    return (seq[n:] + seq[:n])\n",
    "\n",
    "txt=\"abcdefghijklmn\"*100\n",
    "\n",
    "tk = Tokenizer(nb_words=2000, lower=True, split=\" \")\n",
    "tk.fit_on_texts(txt)\n",
    "x = tk.texts_to_sequences(txt)\n",
    "#shifing to left\n",
    "y = shift(x,1)\n",
    "\n",
    "#padding sequence\n",
    "max_len = 100\n",
    "max_features=len(tk.word_counts)\n",
    "X = pad_sequences(x, maxlen=max_len)\n",
    "Y = pad_sequences(y, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0, ...,  0,  0,  1],\n",
       "       [ 0,  0,  0, ...,  0,  0,  2],\n",
       "       [ 0,  0,  0, ...,  0,  0,  3],\n",
       "       ..., \n",
       "       [ 0,  0,  0, ...,  0,  0, 12],\n",
       "       [ 0,  0,  0, ...,  0,  0, 13],\n",
       "       [ 0,  0,  0, ...,  0,  0, 14]], dtype=int32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Pad sequences (samples x time)\n",
      "x_train shape: (25000, 80)\n",
      "x_test shape: (25000, 80)\n",
      "Build model...\n",
      "Train...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 113s - loss: 0.4621 - acc: 0.7769 - val_loss: 0.3976 - val_acc: 0.8348\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 106s - loss: 0.2933 - acc: 0.8797 - val_loss: 0.3862 - val_acc: 0.8398\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 105s - loss: 0.2085 - acc: 0.9198 - val_loss: 0.4187 - val_acc: 0.8275\n",
      "25000/25000 [==============================] - 19s    \n",
      "Test score: 0.418655139561\n",
      "Test accuracy: 0.82752\n"
     ]
    }
   ],
   "source": [
    "'''Trains an LSTM model on the IMDB sentiment classification task.\n",
    "The dataset is actually too small for LSTM to be of any advantage\n",
    "compared to simpler, much faster methods such as TF-IDF + LogReg.\n",
    "Notes:\n",
    "- RNNs are tricky. Choice of batch size is important,\n",
    "choice of loss and optimizer is critical, etc.\n",
    "Some configurations won't converge.\n",
    "- LSTM loss decrease patterns during training can be quite different\n",
    "from what you see with CNNs/MLPs/etc.\n",
    "'''\n",
    "from __future__ import print_function\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.datasets import imdb\n",
    "\n",
    "max_features = 20000\n",
    "maxlen = 80  # cut texts after this number of words (among top max_features most common words)\n",
    "batch_size = 32\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=3,\n",
    "          validation_data=(x_test, y_test))\n",
    "score, acc = model.evaluate(x_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_data = [[1,2,3,4,5,6,7,8,9,10],\n",
    "         [343,43,3,4,5,6,7,8,9,10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3, 4, 5, 6, 7, 8, 9, 10], [343, 43, 3, 4, 5, 6, 7, 8, 9, 10]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_train = sequence.pad_sequences(X_data, maxlen=10)\n",
    "x_test = sequence.pad_sequences([[2,4,6,8,9],\n",
    "         [343,43,8,9,10]],maxlen=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0,   0,   0,   2,   4,   6,   8,   9],\n",
       "       [  0,   0,   0,   0,   0, 343,  43,   8,   9,  10]], dtype=int32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from __future__ import absolute_import\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "from six.moves import range\n",
    "\n",
    "\n",
    "def pad_sequences(sequences, maxlen=None, dtype='int32',\n",
    "                  padding='pre', truncating='pre', value=0.):\n",
    "    \"\"\"Pads each sequence to the same length (length of the longest sequence).\n",
    "    If maxlen is provided, any sequence longer\n",
    "    than maxlen is truncated to maxlen.\n",
    "    Truncation happens off either the beginning (default) or\n",
    "    the end of the sequence.\n",
    "    Supports post-padding and pre-padding (default).\n",
    "    # Arguments\n",
    "        sequences: list of lists where each element is a sequence\n",
    "        maxlen: int, maximum length\n",
    "        dtype: type to cast the resulting sequence.\n",
    "        padding: 'pre' or 'post', pad either before or after each sequence.\n",
    "        truncating: 'pre' or 'post', remove values from sequences larger than\n",
    "            maxlen either in the beginning or in the end of the sequence\n",
    "        value: float, value to pad the sequences to the desired value.\n",
    "    # Returns\n",
    "        x: numpy array with dimensions (number_of_sequences, maxlen)\n",
    "    # Raises\n",
    "        ValueError: in case of invalid values for `truncating` or `padding`,\n",
    "            or in case of invalid shape for a `sequences` entry.\n",
    "    \"\"\"\n",
    "    if not hasattr(sequences, '__len__'):\n",
    "        raise ValueError('`sequences` must be iterable.')\n",
    "    lengths = []\n",
    "    for x in sequences:\n",
    "        if not hasattr(x, '__len__'):\n",
    "            raise ValueError('`sequences` must be a list of iterables. '\n",
    "                             'Found non-iterable: ' + str(x))\n",
    "        lengths.append(len(x))\n",
    "\n",
    "    num_samples = len(sequences)\n",
    "    if maxlen is None:\n",
    "        maxlen = np.max(lengths)\n",
    "\n",
    "    # take the sample shape from the first non empty sequence\n",
    "    # checking for consistency in the main loop below.\n",
    "    sample_shape = tuple()\n",
    "    for s in sequences:\n",
    "        if len(s) > 0:\n",
    "            sample_shape = np.asarray(s).shape[1:]\n",
    "            break\n",
    "\n",
    "    x = (np.ones((num_samples, maxlen) + sample_shape) * value).astype(dtype)\n",
    "    for idx, s in enumerate(sequences):\n",
    "        if not len(s):\n",
    "            continue  # empty list/array was found\n",
    "        if truncating == 'pre':\n",
    "            trunc = s[-maxlen:]\n",
    "        elif truncating == 'post':\n",
    "            trunc = s[:maxlen]\n",
    "        else:\n",
    "            raise ValueError('Truncating type \"%s\" not understood' % truncating)\n",
    "\n",
    "        # check `trunc` has expected shape\n",
    "        trunc = np.asarray(trunc, dtype=dtype)\n",
    "        if trunc.shape[1:] != sample_shape:\n",
    "            raise ValueError('Shape of sample %s of sequence at position %s is different from expected shape %s' %\n",
    "                             (trunc.shape[1:], idx, sample_shape))\n",
    "\n",
    "        if padding == 'post':\n",
    "            x[idx, :len(trunc)] = trunc\n",
    "        elif padding == 'pre':\n",
    "            x[idx, -len(trunc):] = trunc\n",
    "        else:\n",
    "            raise ValueError('Padding type \"%s\" not understood' % padding)\n",
    "    return x\n",
    "\n",
    "\n",
    "def make_sampling_table(size, sampling_factor=1e-5):\n",
    "    \"\"\"Generates a word rank-based probabilistic sampling table.\n",
    "    This generates an array where the ith element\n",
    "    is the probability that a word of rank i would be sampled,\n",
    "    according to the sampling distribution used in word2vec.\n",
    "    The word2vec formula is:\n",
    "        p(word) = min(1, sqrt(word.frequency/sampling_factor) / (word.frequency/sampling_factor))\n",
    "    We assume that the word frequencies follow Zipf's law (s=1) to derive\n",
    "    a numerical approximation of frequency(rank):\n",
    "       frequency(rank) ~ 1/(rank * (log(rank) + gamma) + 1/2 - 1/(12*rank))\n",
    "        where gamma is the Euler-Mascheroni constant.\n",
    "    # Arguments\n",
    "        size: int, number of possible words to sample.\n",
    "        sampling_factor: the sampling factor in the word2vec formula.\n",
    "    # Returns\n",
    "        A 1D Numpy array of length `size` where the ith entry\n",
    "        is the probability that a word of rank i should be sampled.\n",
    "    \"\"\"\n",
    "    gamma = 0.577\n",
    "    rank = np.arange(size)\n",
    "    rank[0] = 1\n",
    "    inv_fq = rank * (np.log(rank) + gamma) + 0.5 - 1. / (12. * rank)\n",
    "    f = sampling_factor * inv_fq\n",
    "\n",
    "    return np.minimum(1., f / np.sqrt(f))\n",
    "\n",
    "\n",
    "def skipgrams(sequence, vocabulary_size,\n",
    "              window_size=4, negative_samples=1., shuffle=True,\n",
    "              categorical=False, sampling_table=None, seed=None):\n",
    "    \"\"\"Generates skipgram word pairs.\n",
    "    Takes a sequence (list of indexes of words),\n",
    "    returns couples of [word_index, other_word index] and labels (1s or 0s),\n",
    "    where label = 1 if 'other_word' belongs to the context of 'word',\n",
    "    and label=0 if 'other_word' is randomly sampled\n",
    "    # Arguments\n",
    "        sequence: a word sequence (sentence), encoded as a list\n",
    "            of word indices (integers). If using a `sampling_table`,\n",
    "            word indices are expected to match the rank\n",
    "            of the words in a reference dataset (e.g. 10 would encode\n",
    "            the 10-th most frequently occurring token).\n",
    "            Note that index 0 is expected to be a non-word and will be skipped.\n",
    "        vocabulary_size: int. maximum possible word index + 1\n",
    "        window_size: int. actually half-window.\n",
    "            The window of a word wi will be [i-window_size, i+window_size+1]\n",
    "        negative_samples: float >= 0. 0 for no negative (=random) samples.\n",
    "            1 for same number as positive samples. etc.\n",
    "        shuffle: whether to shuffle the word couples before returning them.\n",
    "        categorical: bool. if False, labels will be\n",
    "            integers (eg. [0, 1, 1 .. ]),\n",
    "            if True labels will be categorical eg. [[1,0],[0,1],[0,1] .. ]\n",
    "        sampling_table: 1D array of size `vocabulary_size` where the entry i\n",
    "            encodes the probability to sample a word of rank i.\n",
    "        seed: random seed.\n",
    "    # Returns\n",
    "        couples, labels: where `couples` are int pairs and\n",
    "            `labels` are either 0 or 1.\n",
    "    # Note\n",
    "        By convention, index 0 in the vocabulary is\n",
    "        a non-word and will be skipped.\n",
    "    \"\"\"\n",
    "    couples = []\n",
    "    labels = []\n",
    "    for i, wi in enumerate(sequence):\n",
    "        if not wi:\n",
    "            continue\n",
    "        if sampling_table is not None:\n",
    "            if sampling_table[wi] < random.random():\n",
    "                continue\n",
    "\n",
    "        window_start = max(0, i - window_size)\n",
    "        window_end = min(len(sequence), i + window_size + 1)\n",
    "        for j in range(window_start, window_end):\n",
    "            if j != i:\n",
    "                wj = sequence[j]\n",
    "                if not wj:\n",
    "                    continue\n",
    "                couples.append([wi, wj])\n",
    "                if categorical:\n",
    "                    labels.append([0, 1])\n",
    "                else:\n",
    "                    labels.append(1)\n",
    "\n",
    "    if negative_samples > 0:\n",
    "        num_negative_samples = int(len(labels) * negative_samples)\n",
    "        words = [c[0] for c in couples]\n",
    "        random.shuffle(words)\n",
    "\n",
    "        couples += [[words[i % len(words)],\n",
    "                    random.randint(1, vocabulary_size - 1)] for i in range(num_negative_samples)]\n",
    "        if categorical:\n",
    "            labels += [[1, 0]] * num_negative_samples\n",
    "        else:\n",
    "            labels += [0] * num_negative_samples\n",
    "\n",
    "    if shuffle:\n",
    "        if seed is None:\n",
    "            seed = random.randint(0, 10e6)\n",
    "        random.seed(seed)\n",
    "        random.shuffle(couples)\n",
    "        random.seed(seed)\n",
    "        random.shuffle(labels)\n",
    "\n",
    "    return couples, labels\n",
    "\n",
    "\n",
    "def _remove_long_seq(maxlen, seq, label):\n",
    "    \"\"\"Removes sequences that exceed the maximum length.\n",
    "    # Arguments\n",
    "        maxlen: int, maximum length\n",
    "        seq: list of lists where each sublist is a sequence\n",
    "        label: list where each element is an integer\n",
    "    # Returns\n",
    "        new_seq, new_label: shortened lists for `seq` and `label`.\n",
    "    \"\"\"\n",
    "    new_seq, new_label = [], []\n",
    "    for x, y in zip(seq, label):\n",
    "        if len(x) < maxlen:\n",
    "            new_seq.append(x)\n",
    "            new_label.append(y)\n",
    "    return new_seq, new_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sliding_window(df,window_size,window_stride,input_features=None, output_features=None):\n",
    "    allData = df[input_features].values.tolist()\n",
    "    if output_features:\n",
    "        outputData = df[output_features].values.tolist()\n",
    "    print(outputData)\n",
    "    myArray = [[]]\n",
    "    outputDataArray = [] \n",
    "    start=0\n",
    "    for i in range(0,len(allData),window_stride):\n",
    "        #print(allData[i:window_size+i])\n",
    "        print(i)\n",
    "        if i == 0:\n",
    "            myArray = [allData[i:window_size+i]]\n",
    "            if output_features != None:\n",
    "                outputDataArray.append(outputData[window_size+i][0])\n",
    "        else:\n",
    "            myArray.append(allData[i:window_size+i])\n",
    "            \n",
    "            if window_size+i >= len(allData):\n",
    "                if output_features != None:\n",
    "                    outputDataArray.append(outputData[len(allData)-1][0])\n",
    "                break\n",
    "            if output_features != None:\n",
    "                    outputDataArray.append(outputData[window_size+i][0])\n",
    "\n",
    "    if output_features == None:\n",
    "            return (np.array(myArray))\n",
    "    print(outputDataArray)   \n",
    "    return (np.array(myArray), outputDataArray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Get some time series data\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/plotly/datasets/master/timeseries.csv\")\n",
    "df['output']=[1,0,1,0,1,0,1,0,1,0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     1\n",
       "1     0\n",
       "2     1\n",
       "3     0\n",
       "4     1\n",
       "5     0\n",
       "6     1\n",
       "7     0\n",
       "8     1\n",
       "9     0\n",
       "10    1\n",
       "Name: output, dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1]]\n",
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "[0, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "input_cols = ['A','B','C']\n",
    "x_train,y_train=sliding_window(df,5,2,input_cols,['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[  24.68,  164.93,  114.73],\n",
       "        [  24.18,  164.89,  114.75],\n",
       "        [  23.99,  164.63,  115.04],\n",
       "        [  24.14,  163.92,  114.85],\n",
       "        [  24.44,  163.45,  114.84]],\n",
       "\n",
       "       [[  23.99,  164.63,  115.04],\n",
       "        [  24.14,  163.92,  114.85],\n",
       "        [  24.44,  163.45,  114.84],\n",
       "        [  24.38,  163.46,  115.4 ],\n",
       "        [  24.32,  163.22,  115.56]],\n",
       "\n",
       "       [[  24.44,  163.45,  114.84],\n",
       "        [  24.38,  163.46,  115.4 ],\n",
       "        [  24.32,  163.22,  115.56],\n",
       "        [  24.19,  164.02,  115.54],\n",
       "        [  23.81,  163.59,  115.72]],\n",
       "\n",
       "       [[  24.32,  163.22,  115.56],\n",
       "        [  24.19,  164.02,  115.54],\n",
       "        [  23.81,  163.59,  115.72],\n",
       "        [  24.03,  163.32,  115.11],\n",
       "        [  24.34,  163.34,  115.17]]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#y_train = np.array(y_train)\n",
    "y_train = np.array(y_train).reshape((-1, 1))\n",
    "y_train = to_categorical(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mygpu/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:6: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(128, return_sequences=True, input_shape=(5, 3), activation=\"sigmoid\", recurrent_activation=\"hard_sigmoid\")`\n",
      "/home/mygpu/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:13: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(units=2, activation=\"sigmoid\", kernel_initializer=\"uniform\")`\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "# input_shape = number of time-steps, number-of-features\n",
    "model.add(LSTM(128,input_shape=(5,3),\n",
    "               activation='sigmoid', \n",
    "               inner_activation='hard_sigmoid', \n",
    "               return_sequences=True))\n",
    "model.add(LSTM(128, activation='tanh', recurrent_activation='hard_sigmoid'))\n",
    "#model.add(Activation('sigmoid'))\n",
    "model.add(Dropout(0.2))\n",
    "#model.add(TimeDistributedDense(11))\n",
    "#model.add(Dense(128))\n",
    "model.add(Dense(64, kernel_initializer='uniform', activation='relu'))\n",
    "model.add(Dense(output_dim=2, kernel_initializer='uniform', activation='sigmoid'))\n",
    "#model.add(Activation('sigmoid'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_15 (LSTM)               (None, 5, 128)            67584     \n",
      "_________________________________________________________________\n",
      "lstm_16 (LSTM)               (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 207,554\n",
      "Trainable params: 207,554\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 4 samples, validate on 4 samples\n",
      "Epoch 1/5\n",
      "4/4 [==============================] - 0s - loss: 0.5973 - val_loss: 0.5623\n",
      "Epoch 2/5\n",
      "4/4 [==============================] - 0s - loss: 0.5749 - val_loss: 0.5623\n",
      "Epoch 3/5\n",
      "4/4 [==============================] - 0s - loss: 0.5987 - val_loss: 0.5624\n",
      "Epoch 4/5\n",
      "4/4 [==============================] - 0s - loss: 0.5800 - val_loss: 0.5624\n",
      "Epoch 5/5\n",
      "4/4 [==============================] - 0s - loss: 0.5977 - val_loss: 0.5624\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb9b6edbac8>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=1,\n",
    "          epochs=5,\n",
    "          validation_data=(x_train, y_train))\n",
    "#score, acc = \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/4 [======>.......................] - ETA: 0s\n",
      "Test score: 0.562384031713\n",
      "Test accuracy: 0.82752\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_train, y_train,batch_size=1)\n",
    "print()\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.74791425,  0.24362557],\n",
       "       [ 0.74791294,  0.2436263 ],\n",
       "       [ 0.74791336,  0.24362645],\n",
       "       [ 0.74791366,  0.24362709]], dtype=float32)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  24.68,  164.93,  114.73],\n",
       "       [  24.18,  164.89,  114.75],\n",
       "       [  23.99,  164.63,  115.04],\n",
       "       [  24.14,  163.92,  114.85],\n",
       "       [  24.44,  163.45,  114.84]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  24.18,  164.89,  114.75],\n",
       "       [  23.99,  164.63,  115.04],\n",
       "       [  24.14,  163.92,  114.85],\n",
       "       [  24.44,  163.45,  114.84],\n",
       "       [  24.38,  163.46,  115.4 ]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[  24.18,  164.89,  114.75],\n",
       "        [  23.99,  164.63,  115.04],\n",
       "        [  24.14,  163.92,  114.85],\n",
       "        [  24.44,  163.45,  114.84],\n",
       "        [  24.38,  163.46,  115.4 ]]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[  24.18,  164.89,  114.75],\n",
       "        [  23.99,  164.63,  115.04],\n",
       "        [  24.14,  163.92,  114.85],\n",
       "        [  24.44,  163.45,  114.84],\n",
       "        [  24.38,  163.46,  115.4 ]])]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[]], array([[  24.18,  164.89,  114.75],\n",
       "        [  23.99,  164.63,  115.04],\n",
       "        [  24.14,  163.92,  114.85],\n",
       "        [  24.44,  163.45,  114.84],\n",
       "        [  24.38,  163.46,  115.4 ]]), [[24.68, 164.93, 114.73],\n",
       "  [24.18, 164.89, 114.75],\n",
       "  [23.99, 164.63, 115.04],\n",
       "  [24.14, 163.92, 114.85],\n",
       "  [24.44, 163.45, 114.84],\n",
       "  [24.38, 163.46, 115.4],\n",
       "  [24.32, 163.22, 115.56],\n",
       "  [24.19, 164.02, 115.54],\n",
       "  [23.81, 163.59, 115.72],\n",
       "  [24.03, 163.32, 115.11],\n",
       "  [24.34, 163.34, 115.17]]]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[24.68, 164.93, 114.73],\n",
       " [24.18, 164.89, 114.75],\n",
       " [23.99, 164.63, 115.04],\n",
       " [24.14, 163.92, 114.85],\n",
       " [24.44, 163.45, 114.84],\n",
       " [24.38, 163.46, 115.4],\n",
       " [24.32, 163.22, 115.56],\n",
       " [24.19, 164.02, 115.54],\n",
       " [23.81, 163.59, 115.72],\n",
       " [24.03, 163.32, 115.11],\n",
       " [24.34, 163.34, 115.17]]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a=[[[]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a[0] = ALL[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a.append(ALL[1:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[24.68, 164.93, 114.73],\n",
       "  [24.18, 164.89, 114.75],\n",
       "  [23.99, 164.63, 115.04],\n",
       "  [24.14, 163.92, 114.85],\n",
       "  [24.44, 163.45, 114.84]],\n",
       " [[24.18, 164.89, 114.75],\n",
       "  [23.99, 164.63, 115.04],\n",
       "  [24.14, 163.92, 114.85],\n",
       "  [24.44, 163.45, 114.84],\n",
       "  [24.38, 163.46, 115.4]]]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[  24.68,  164.93,  114.73],\n",
       "        [  24.18,  164.89,  114.75],\n",
       "        [  23.99,  164.63,  115.04],\n",
       "        [  24.14,  163.92,  114.85],\n",
       "        [  24.44,  163.45,  114.84]],\n",
       "\n",
       "       [[  24.18,  164.89,  114.75],\n",
       "        [  23.99,  164.63,  115.04],\n",
       "        [  24.14,  163.92,  114.85],\n",
       "        [  24.44,  163.45,  114.84],\n",
       "        [  24.38,  163.46,  115.4 ]]])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ALL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
