{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import argparse\n",
    "\n",
    "import gensim\n",
    "from gensim.models import word2vec\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import LSTM, Bidirectional\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "from keras.utils import to_categorical\n",
    "#x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "from keras import backend as K\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from sklearn import preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sliding_window(df,window_size,window_stride,input_features=None, output_features=None):\n",
    "    allData = df[input_features].values.tolist()\n",
    "    if output_features:\n",
    "        outputData = [i[0] for i in df[output_features].values.tolist()]\n",
    "    myArray = [[]]\n",
    "    outputDataArray = [] \n",
    "    \n",
    "    start=0\n",
    "    for i in range(0,len(allData),window_stride):\n",
    "        if i == 0:\n",
    "            myArray = [allData[i:window_size+i]]\n",
    "            if output_features != None:\n",
    "                #outputDataArray.append(outputData[window_size+i][0])\n",
    "                outputDataArray=[outputData[i+window_size-1]]\n",
    "                #print(1,type(outputDataArray))\n",
    "        else:\n",
    "            myArray.append(allData[i:window_size+i])\n",
    "            \n",
    "            if window_size+i >= len(allData):\n",
    "                if output_features != None:\n",
    "                    #print(2,type(outputDataArray))\n",
    "                    outputDataArray.append(outputData[len(allData)-1])\n",
    "                break\n",
    "            if output_features != None:\n",
    "                    #outputDataArray.append(outputData[window_size+i][0])\n",
    "                    #print(3,type(outputDataArray))\n",
    "                    outputDataArray.append(outputData[i+window_size-1])\n",
    "\n",
    "    if output_features == None:\n",
    "            return (np.array(myArray))\n",
    "    print(np.shape(np.array(myArray)))\n",
    "    #return (np.array(myArray), outputDataArray)\n",
    "    return (myArray, outputDataArray)\n",
    "\n",
    "def mystratifiedOutputSampler(df, target):\n",
    "    #a = {i:[] for i in df[target].unique()}\n",
    "    xTIndexes = []\n",
    "    minLen = None\n",
    "    #print(a)\n",
    "    for i in df[target].unique():\n",
    "        #a[i] = df.index[df[target] != i].tolist()\n",
    "        a = df.index[df[target] != i].tolist()\n",
    "        print(i,len(a))\n",
    "        if minLen == None or minLen > len(a):\n",
    "            minLen = len(a)\n",
    "    #for i in a.keys():\n",
    "    print(minLen)\n",
    "    for i in df[target].unique():\n",
    "        if not xTIndexes:\n",
    "                print(2)\n",
    "                xTIndexes = list(np.random.choice(df.index[df[target] != i].tolist(), size=minLen, replace=False))\n",
    "        else:\n",
    "            #print(OnlyNeutal)\n",
    "            xTIndexes = xTIndexes + list(np.random.choice(df.index[df[target] != i].tolist(), size=minLen, replace=False))\n",
    "\n",
    "    return (xTIndexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def fileProcessor(csvFile,stockName):\n",
    "    dateparse = lambda x: pd.datetime.strptime(x, '%d-%m-%Y')\n",
    "    df = pd.read_csv(csvFile, parse_dates=['date'],date_parser=dateparse)\n",
    "    #df = pd.read_csv(\"/datadrive/Sahil/code/GL/fewTrails/Datasets/GE.csv\", parse_dates=['date'],date_parser=dateparse)\n",
    "\n",
    "\n",
    "    df[\"date\"]  = pd.to_datetime(df.date)\n",
    "    #type(df[\"date\"].iloc[0])\n",
    "    df = df.sort_values(by=\"date\")\n",
    "\n",
    "    df['close_delta'] = 0\n",
    "    df['close_direction'] = 0\n",
    "    df['Stock_name'] = stockName\n",
    "    for index in range(0,df.shape[0]):\n",
    "        #print(index,df.iloc[index]['close'])\n",
    "        if index == 0:\n",
    "            df['close_delta'].iloc[index] =  (float(0))\n",
    "            df['close_direction'].iloc[index] =  1\n",
    "        #elif index <= df.shape[0]-1:\n",
    "        else:\n",
    "            df['close_delta'].iloc[index] = ((df['close'].iloc[index] - df['close'].iloc[index-1])/df['close'].iloc[index-1])*100\n",
    "            if df['close_delta'].iloc[index] >= 1:\n",
    "                df['close_direction'].iloc[index] = 2\n",
    "            elif df['close_delta'].iloc[index] <= -1:\n",
    "                df['close_direction'].iloc[index] = 0\n",
    "            else:\n",
    "                df['close_direction'].iloc[index] = 1\n",
    "    return (df)\n",
    "'''\n",
    "fileCount = 0\n",
    "import os\n",
    "for root, dirs, files in os.walk(\"/datadrive/Sahil/code/GL/fewTrails/Datasets\"):\n",
    "    for file in files:\n",
    "        if file.endswith(\".csv\"):\n",
    "            print(os.path.join(root, file))\n",
    "            if fileCount == 0:\n",
    "                df = fileProcessor(os.path.join(root, file),file.split('.')[0])\n",
    "            else:\n",
    "                df = df.append(fileProcessor(os.path.join(root, file),file.split('.')[0]))\n",
    "                \n",
    "'''    \n",
    "gCount=0\n",
    "#df.apply(lambda x: featureTransform(x,df.shape[0]),axis=1)\n",
    "#df.shape[0]\n",
    "dateparse = lambda x: pd.datetime.strptime(x, '%Y-%m-%d')\n",
    "csvFile = \"/datadrive/Sahil/code/GL/fewTrails/Datasets/ALL_clean.csv\"\n",
    "df = pd.read_csv(csvFile, parse_dates=['date'],date_parser=dateparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2693\n",
      "0 7446\n",
      "2 7175\n",
      "2693\n",
      "2\n",
      "(8079, 1, 6)\n",
      "(8079, 1, 6)\n",
      "GE\n",
      "(1148, 1, 6)\n",
      "APPLE\n",
      "(1093, 1, 6)\n",
      "<class 'list'> <class 'list'> (1148, 1, 6)\n",
      "IBM\n",
      "(1126, 1, 6)\n",
      "<class 'list'> <class 'list'> (2241, 1, 6)\n",
      "MCD\n",
      "(1058, 1, 6)\n",
      "<class 'list'> <class 'list'> (3367, 1, 6)\n",
      "BOEING\n",
      "(1266, 1, 6)\n",
      "<class 'list'> <class 'list'> (4425, 1, 6)\n",
      "MICROSOFT\n",
      "(1200, 1, 6)\n",
      "<class 'list'> <class 'list'> (5691, 1, 6)\n",
      "DISNEY\n",
      "(1188, 1, 6)\n",
      "<class 'list'> <class 'list'> (6891, 1, 6)\n"
     ]
    }
   ],
   "source": [
    "window_size = 1\n",
    "input_cols = ['curr_ratio','tot_debt_tot_equity', 'oper_profit_margin','asset_turn','ret_equity','sentiment']\n",
    "\n",
    "input_cols_scale = ['curr_ratio','tot_debt_tot_equity', 'oper_profit_margin','asset_turn','ret_equity',]\n",
    "\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "#df_all.reset_index(drop=True)\n",
    "mydf =  pd.DataFrame(preprocessing.scale(df[input_cols_scale]),columns=input_cols_scale)\n",
    "mydf['sentiment'] = df['sentiment']\n",
    "mydf['close_direction'] = df['close_direction']\n",
    "mydf['Stock_name'] = df['Stock_name']\n",
    "train_indexes = mystratifiedOutputSampler(mydf,'close_direction')\n",
    "mydf = mydf.iloc[train_indexes]\n",
    "\n",
    "x_train,y_train=sliding_window(mydf,window_size,1,input_cols,['close_direction'])\n",
    "x_train = np.array(x_train)\n",
    "print(np.shape(x_train))\n",
    "for index,stock in enumerate(mydf['Stock_name'].unique()):\n",
    "    print(stock)\n",
    "    if index == 0:\n",
    "        x_train,y_train=sliding_window(mydf[mydf['Stock_name'] == stock],window_size,1,input_cols,['close_direction'])\n",
    "    else:\n",
    "        x_,y_=sliding_window(mydf[mydf['Stock_name'] == stock],window_size,1,input_cols,['close_direction'])\n",
    "        print(type(x_train),type(x_),np.shape(x_train))\n",
    "        x_train = x_train + x_\n",
    "        y_train = y_train + y_\n",
    "x_train = np.array(x_train)\n",
    "#y_train = np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_train1 = np.array(y_train)\n",
    "y_train1 = np.array(y_train1).reshape((-1, 1))\n",
    "#y_train = to_categorical(y_train)\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "#integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "y_train = onehot_encoder.fit_transform(y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(y_train)[1]\n",
    "#onehot_encoder.transform(y_train1[0]).tolist()[0]\n",
    "#for i in onehot_encoder.transform(y_train1):\n",
    "#    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def getBalancedSamples(x_train,proportionBYNonNeutal=.7,neutalProportionToOther=.7):\n",
    "    #proportionBYNonNeutal = .7\n",
    "    #neutalProportionToOther = .7\n",
    "    AllWithAnyOtherThanNeutal = []\n",
    "    OnlyNeutal=[]\n",
    "    #for index,i in enumerate(x_train[1:500,:,:]):\n",
    "    for index,i in enumerate(x_train[:,:,:]):\n",
    "        #print()\n",
    "        #print(i)\n",
    "        #print(i[:,5])\n",
    "        if 1 in i[:,5] or 2 in i[:,5] or 4 in i[:,5] or 5 in i[:,5]  :\n",
    "            #print(\"TRUE\")\n",
    "            if not AllWithAnyOtherThanNeutal:\n",
    "                AllWithAnyOtherThanNeutal = [index]\n",
    "            else:\n",
    "                AllWithAnyOtherThanNeutal.append(index)\n",
    "        else:\n",
    "            if not OnlyNeutal:\n",
    "                print(2)\n",
    "                OnlyNeutal = [index]\n",
    "            else:\n",
    "                #print(OnlyNeutal)\n",
    "                OnlyNeutal.append(index)\n",
    "    print (len(AllWithAnyOtherThanNeutal))\n",
    "    print (len(OnlyNeutal))\n",
    "    #AllWithAnyOtherThanNeutal[np.random.randint(0,100,size=20)]\n",
    "    proportion = .7\n",
    "    size = int(round(proportion*len(AllWithAnyOtherThanNeutal),0))\n",
    "    print(size)\n",
    "    #xTIndexesPart1 = [AllWithAnyOtherThanNeutal[i] for i in list(np.random.randint(0,100,size=size))]\n",
    "    #xTIndexesPart2 = [OnlyNeutal[i] for i in list(np.random.randint(0,100,size=int(round(size*.7)))) ]\n",
    "    #print(len(xTIndexesPart1),len(xTIndexesPart2))\n",
    "    #xTIndexes = AllWithAnyOtherThanNeutal + OnlyNeutal\n",
    "    \n",
    "    #X_train, X_test, y_train, y_test = train_test_split(x_train[xTIndexes,:,:], y[xTIndexes], test_size=0.2)\n",
    "    xTIndexesPart1 = list(np.random.choice(AllWithAnyOtherThanNeutal, size=size, replace=False))\n",
    "    xTIndexesPart2 = list(np.random.choice(OnlyNeutal, size=int(round(size*.7)), replace=False))\n",
    "    xTIndexes = xTIndexesPart1 + xTIndexesPart2\n",
    "    print(len(xTIndexes))\n",
    "    xTIndexesPart1 = list(np.random.choice(xTIndexes, size=len(xTIndexes), replace=False))\n",
    "    #xTIndexes = np.sort(xTIndexes)\n",
    "    return(xTIndexes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 2])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['close_direction'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "475\n",
      "7604\n",
      "332\n",
      "564\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       ..., \n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  0.,  1.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#y_train\n",
    "train_indexes = getBalancedSamples(x_train)\n",
    "X_train = x_train[train_indexes]\n",
    "Y_train = y_train[train_indexes]\n",
    "\n",
    "#X_train = x_train\n",
    "#Y_train = y_train\n",
    "Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mygpu/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:8: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(32, activation=\"relu\", recurrent_activation=\"hard_sigmoid\", return_sequences=True, input_shape=(1, 6))`\n",
      "/home/mygpu/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:20: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"softmax\", units=3)`\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import seed\n",
    "seed(1)\n",
    "model = Sequential()\n",
    "# input_shape = number of time-steps, number-of-features\n",
    "model.add(LSTM(32,input_shape=(window_size,len(input_cols)),\n",
    "               activation='relu', \n",
    "               inner_activation='hard_sigmoid', \n",
    "               return_sequences=True))\n",
    "model.add(LSTM(32, activation='relu', recurrent_activation='hard_sigmoid'))\n",
    "#model.add(Activation('sigmoid'))\n",
    "model.add(Dropout(0.1))\n",
    "#model.add(TimeDistributedDense(11))\n",
    "#model.add(Dense(128))\n",
    "#model.add(Dense(128,activation='relu'))\n",
    "#model.add(Dropout(0.5))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(output_dim=np.shape(y_train)[1], activation='softmax'))\n",
    "#model.add(Activation('sigmoid'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_13 (LSTM)               (None, 1, 32)             4992      \n",
      "_________________________________________________________________\n",
      "lstm_14 (LSTM)               (None, 32)                8320      \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 3)                 51        \n",
      "=================================================================\n",
      "Total params: 14,947\n",
      "Trainable params: 14,947\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 564 samples, validate on 564 samples\n",
      "Epoch 1/30\n",
      "564/564 [==============================] - 7s - loss: 1.0787 - val_loss: 1.0700\n",
      "Epoch 2/30\n",
      "564/564 [==============================] - 6s - loss: 1.0776 - val_loss: 1.0674\n",
      "Epoch 3/30\n",
      "564/564 [==============================] - 6s - loss: 1.0709 - val_loss: 1.0695\n",
      "Epoch 4/30\n",
      "564/564 [==============================] - 6s - loss: 1.0769 - val_loss: 1.0602\n",
      "Epoch 5/30\n",
      "564/564 [==============================] - 6s - loss: 1.0678 - val_loss: 1.0583\n",
      "Epoch 6/30\n",
      "564/564 [==============================] - 6s - loss: 1.0716 - val_loss: 1.0438\n",
      "Epoch 7/30\n",
      "564/564 [==============================] - 6s - loss: 1.0576 - val_loss: 1.0304\n",
      "Epoch 8/30\n",
      "564/564 [==============================] - 6s - loss: 1.0514 - val_loss: 1.0222\n",
      "Epoch 9/30\n",
      "564/564 [==============================] - 6s - loss: 1.0365 - val_loss: 1.0046\n",
      "Epoch 10/30\n",
      "564/564 [==============================] - 6s - loss: 1.0208 - val_loss: 1.0280\n",
      "Epoch 11/30\n",
      "564/564 [==============================] - 6s - loss: 1.0240 - val_loss: 0.9937\n",
      "Epoch 12/30\n",
      "564/564 [==============================] - 6s - loss: 1.0108 - val_loss: 0.9796\n",
      "Epoch 13/30\n",
      "564/564 [==============================] - 6s - loss: 1.0159 - val_loss: 0.9704\n",
      "Epoch 14/30\n",
      "564/564 [==============================] - 6s - loss: 1.0033 - val_loss: 0.9739\n",
      "Epoch 15/30\n",
      "564/564 [==============================] - 6s - loss: 1.0011 - val_loss: 0.9511\n",
      "Epoch 16/30\n",
      "564/564 [==============================] - 6s - loss: 1.0121 - val_loss: 0.9570\n",
      "Epoch 17/30\n",
      "564/564 [==============================] - 6s - loss: 1.0314 - val_loss: 0.9790\n",
      "Epoch 18/30\n",
      "564/564 [==============================] - 6s - loss: 1.0008 - val_loss: 0.9601\n",
      "Epoch 19/30\n",
      "564/564 [==============================] - 6s - loss: 1.0250 - val_loss: 0.9411\n",
      "Epoch 20/30\n",
      "564/564 [==============================] - 6s - loss: 0.9813 - val_loss: 0.9771\n",
      "Epoch 21/30\n",
      "564/564 [==============================] - 6s - loss: 0.9891 - val_loss: 0.9315\n",
      "Epoch 22/30\n",
      "564/564 [==============================] - 6s - loss: 1.0218 - val_loss: 0.9316\n",
      "Epoch 23/30\n",
      "564/564 [==============================] - 6s - loss: 0.9886 - val_loss: 0.9405\n",
      "Epoch 24/30\n",
      "564/564 [==============================] - 6s - loss: 0.9696 - val_loss: 0.9350\n",
      "Epoch 25/30\n",
      "564/564 [==============================] - 6s - loss: 1.0183 - val_loss: 0.9324\n",
      "Epoch 26/30\n",
      "564/564 [==============================] - 6s - loss: 0.9922 - val_loss: 0.9184\n",
      "Epoch 27/30\n",
      "564/564 [==============================] - 6s - loss: 0.9923 - val_loss: 0.9318\n",
      "Epoch 28/30\n",
      "564/564 [==============================] - 6s - loss: 0.9709 - val_loss: 0.9250\n",
      "Epoch 29/30\n",
      "564/564 [==============================] - 6s - loss: 0.9890 - val_loss: 0.9135\n",
      "Epoch 30/30\n",
      "564/564 [==============================] - 6s - loss: 0.9645 - val_loss: 0.9416\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb7fee07630>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Train...')\n",
    "model.fit(X_train, Y_train,\n",
    "          batch_size=1,\n",
    "          epochs=30,\n",
    "          validation_data=(X_train, Y_train))\n",
    "#score, acc = \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "552/564 [============================>.] - ETA: 0s\n",
      "Test score: 0.941588092085\n",
      "[[  7.00256348e-01   2.55791783e-01   4.39519472e-02]\n",
      " [  4.51191701e-03   4.61902693e-02   9.49297845e-01]\n",
      " [  9.94346797e-01   5.55016613e-03   1.03055667e-04]\n",
      " ..., \n",
      " [  1.55191019e-01   5.79142272e-01   2.65666693e-01]\n",
      " [  1.82182699e-01   5.41796923e-01   2.76020348e-01]\n",
      " [  1.86212851e-10   3.61490515e-06   9.99996424e-01]]\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_train, Y_train,batch_size=1)\n",
    "print()\n",
    "print('Test score:', score)\n",
    "print(model.predict(X_train))\n",
    "#print('Test accuracy:', confusion_matrix(Y_train,model.predict_classes(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 32/564 [>.............................] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "#model.predict(x_train)\n",
    "a = model.predict_classes(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 0, 1, 1, 1, 1, 2, 1, 0, 1, 1, 2, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0,\n",
       "       2, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1,\n",
       "       0, 2, 1, 1, 1, 1, 1, 0, 2, 1, 2, 1, 2, 1, 1, 1, 1, 2, 1, 0, 1, 1, 1,\n",
       "       0, 0, 1, 2, 2, 2, 2, 2, 1, 2, 0, 1, 2, 2, 1, 1, 2, 1, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 2, 1, 1, 1, 1, 0, 1, 2, 2, 1, 1, 1, 2, 1, 1, 2,\n",
       "       1, 1, 1, 1, 1, 1, 0, 1, 1, 2, 1, 1, 1, 1, 2, 0, 1, 1, 1, 0, 1, 0, 1,\n",
       "       1, 0, 2, 1, 1, 1, 1, 1, 2, 1, 1, 0, 0, 2, 1, 2, 1, 2, 2, 0, 1, 1, 1,\n",
       "       1, 1, 2, 1, 1, 0, 2, 1, 1, 1, 1, 2, 1, 2, 2, 2, 2, 1, 0, 2, 1, 1, 1,\n",
       "       2, 1, 1, 1, 2, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 2, 2, 1,\n",
       "       1, 2, 0, 2, 2, 1, 2, 0, 2, 1, 2, 2, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,\n",
       "       1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 2, 0, 1, 0,\n",
       "       1, 2, 1, 1, 2, 2, 2, 2, 2, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 2, 2, 0, 1,\n",
       "       1, 1, 1, 0, 1, 1, 2, 2, 1, 0, 1, 1, 1, 2, 1, 0, 1, 2, 2, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 2, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 2, 1,\n",
       "       1, 2, 1, 1, 2, 1, 1, 1, 0, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = set(a)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_train\n",
    "#list(y_train1)\n",
    "#onehot_encoder = OneHotEncoder(sparse=False)\n",
    "onehot_encoder.active_features_\n",
    "labels = Y_train.dot(onehot_encoder.active_features_).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6046099290780141\n"
     ]
    }
   ],
   "source": [
    "accuracy = len(np.where(a == labels)[0].tolist())/len(labels)\n",
    "print (accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 43,   3,   1],\n",
       "       [102, 237,  99],\n",
       "       [  7,  11,  61]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(a.tolist(),labels.tolist())\n",
    "\n",
    "## Older model with tanh and 128,128 LSTM layers\n",
    "# 52% accuracy\n",
    "#array([[ 55,  13,   4],\n",
    "#       [ 63, 134,  77],\n",
    "#       [ 31,  81, 103]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 735,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 55,  13,   4],\n",
       "       [ 63, 134,  77],\n",
       "       [ 31,  81, 103]])"
      ]
     },
     "execution_count": 735,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(a.tolist(),labels.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 736,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output = pd.DataFrame(columns=['predicted','actual'])\n",
    "output['predicted'] = a.tolist()\n",
    "output['actual'] = labels.tolist()\n",
    "output['Stock'] = list(mydf['Stock_name'].iloc[train_indexes])\n",
    "output['Sentiment'] = list(mydf['sentiment'].iloc[train_indexes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 746,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Stock\n",
       "APPLE        1.098592\n",
       "BOEING       1.402439\n",
       "DISNEY       1.294872\n",
       "GE           1.283951\n",
       "IBM          1.225000\n",
       "MCD          1.241379\n",
       "MICROSOFT    1.219512\n",
       "Name: predicted, dtype: float64"
      ]
     },
     "execution_count": 746,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.groupby(by=\"Stock\")['predicted'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#mydf.iloc[train_indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 740,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'SeriesGroupBy' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-740-be37372a81cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maggregate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'actual'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/mygpu/anaconda3/lib/python3.5/site-packages/pandas/core/groupby.py\u001b[0m in \u001b[0;36maggregate\u001b[0;34m(self, arg, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3595\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mAppender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSelectionMixin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_agg_doc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3596\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0maggregate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3597\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDataFrameGroupBy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maggregate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3598\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3599\u001b[0m     \u001b[0magg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maggregate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mygpu/anaconda3/lib/python3.5/site-packages/pandas/core/groupby.py\u001b[0m in \u001b[0;36maggregate\u001b[0;34m(self, arg, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3113\u001b[0m         \u001b[0m_level\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_level'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3114\u001b[0;31m         \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_aggregate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_level\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_level\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3115\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhow\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3116\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mygpu/anaconda3/lib/python3.5/site-packages/pandas/core/base.py\u001b[0m in \u001b[0;36m_aggregate\u001b[0;34m(self, arg, *args, **kwargs)\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0m_level\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_level'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'SeriesGroupBy' object is not callable"
     ]
    }
   ],
   "source": [
    "d.aggregate('actual')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a=[[[]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a[0] = ALL[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a.append(ALL[1:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.array(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(ALL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
